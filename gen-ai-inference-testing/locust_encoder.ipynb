{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locust Load Testing for Encoder Models\n",
    "\n",
    "This notebook provides testing for various types of encoder models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine requirements\n",
    "\n",
    "This notebook should be run on a machine with at least 1 Nvidia GPU, or 1 AWS AI chip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "p = subprocess.run('nvidia-smi --list-gpus | wc -l', \n",
    "                   shell=True, check=True, capture_output=True, encoding='utf-8')\n",
    "\n",
    "device = None\n",
    "num_device = 0\n",
    "\n",
    "if p.returncode == 0:\n",
    "    num_device = int(p.stdout)\n",
    "    device = \"cuda\" if num_device > 0 else None\n",
    "\n",
    "if device is None:\n",
    "    p = subprocess.run('neuron-ls -j | grep neuron_device | wc -l', \n",
    "                       shell=True, check=True, capture_output=True, encoding='utf-8')\n",
    "    if p.returncode == 0:\n",
    "        num_device = int(p.stdout)\n",
    "        device = \"neuron\" if num_device > 0 else None\n",
    "\n",
    "assert (device == \"cuda\" and num_device >= 1) or (device == \"neuron\" and num_device >= 1), \\\n",
    "    \"Machine must have 1 Nvidia CUDA devices, or 1 AWS Neuorn Devices\"\n",
    "print(f\"Auto detected {num_device} {device} devices\")\n",
    "os.environ['NUM_DEVICE']=str(num_device)\n",
    "os.environ['DEVICE']=device\n",
    "\n",
    "import boto3\n",
    "session = boto3.session.Session()\n",
    "os.environ[\"AWS_DEFAULT_REGION\"]=session.region_name\n",
    "assert os.environ[\"AWS_DEFAULT_REGION\"], \"AWS_DEFAULT_REGION environment variable must be set\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, install required Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install locust\n",
    "!pip install datasets\n",
    "!which locust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Hugging Face User Access Token\n",
    "\n",
    "Many of the popular Large Language Models (LLMs) in Hugging Face are [gated models](https://huggingface.co/docs/hub/en/models-gated). To access gated models, you need a Hugging Face [user access token](https://huggingface.co/docs/hub/en/security-tokens). Please create a Hugging Face user access token in your Hugging Face account, and set it below in `hf_token` variable below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import stat\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "hf_token=''\n",
    "# Comment out next line if not using a Hugging Face gated model\n",
    "assert hf_token, \"Hugging Face user access token is required for gated models\"\n",
    "os.environ['HF_TOKEN']=hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Docker Containers\n",
    "\n",
    "Next, we build the docker containers used to run the inference endpoint locally on this desktop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bash scripts/build-containers.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Hugging Face Model Id\n",
    "\n",
    "Next, set the Hugging Face Model Id for the embeddings model you want to test in `hf_model_id` variable, below. \n",
    "\n",
    "The variable `MAX_MODEL_LEN` should be set to the *minimum* of the maximum context length allowed by the model, and the maximum context length you want to use for your testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_id = 'BAAI/bge-reranker-large' # used for reranker\n",
    "encoder_type=\"reranker\"\n",
    "\n",
    "# hf_model_id = 'BAAI/bge-m3' # embeddings\n",
    "# encoder_type=\"embeddings\"\n",
    "\n",
    "# hf_model_id = 'cross-encoder/quora-roberta-base' # sequence_classification\n",
    "# encoder_type=\"sequence_classification\"\n",
    "\n",
    "# hf_model_id = 'dslim/bert-base-NER' # token_classification\n",
    "# encoder_type = \"token_classification\"\n",
    "\n",
    "# hf_model_id = 'sentence-transformers/all-distilroberta-v1' # masked_lm\n",
    "# encoder_type = \"masked_lm\"\n",
    "\n",
    "assert encoder_type in [\"embeddings\", \n",
    "                        \"reranker\",\n",
    "                        \"masked_lm\",\n",
    "                        \"token_classification\",\n",
    "                        \"sequence_classification\"]\n",
    "\n",
    "os.environ['ENCODER_TYPE']=encoder_type\n",
    "os.environ['HF_MODEL_ID']=hf_model_id\n",
    "os.environ['MAX_MODEL_LEN']=str(512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snapshot Huggingface model\n",
    "\n",
    "Below we snapshot the Huggingface model and store it on the EFS. This is only done once. To force a refresh of the model from Huggingface hub, you must delete the local copy of the model from the EFS.\n",
    "\n",
    "To use EFS, we create a symbolic link from `/home/ubuntu/snapshots` to `/home/ubuntu/efs/home/snapshots` directory. Please ensure `/home/ubuntu/efs/home` exists and is owned by user `ubuntu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download, list_repo_files\n",
    "from tempfile import TemporaryDirectory\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import pwd\n",
    "\n",
    "home_dir = os.path.join(os.getenv('HOME'))\n",
    "efs_home = os.path.join(home_dir, \"efs\", \"home\")\n",
    "\n",
    "assert os.path.isdir(efs_home), f\"{efs_home} directory must exist\"\n",
    "\n",
    "stat_info = os.stat(efs_home)\n",
    "owner_uid = stat_info.st_uid\n",
    "owner_username = pwd.getpwuid(owner_uid).pw_name\n",
    "assert owner_username == \"ubuntu\", f\"{efs_home} must be owned by ubuntu\"\n",
    "efs_snapshots = os.path.join(efs_home, \"snapshots\")\n",
    "os.makedirs(efs_snapshots, exist_ok=True)\n",
    "if not os.path.exists(os.path.join(home_dir, \"snapshots\")):\n",
    "    os.symlink(efs_snapshots, os.path.join(home_dir, \"snapshots\")) # create a symbolic link to EFS directory\n",
    "\n",
    "hf_home = os.path.join(home_dir, \"snapshots\", \"huggingface\")\n",
    "os.makedirs(hf_home, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(hf_home, hf_model_id)\n",
    "\n",
    "hub_files = set(list_repo_files(repo_id=hf_model_id, repo_type=\"model\"))\n",
    "# Get local files (recursively)\n",
    "local_files = set()\n",
    "missing_files=None\n",
    "for root, dirs, files in os.walk(model_path):\n",
    "    for file in files:\n",
    "        # Get relative path from model root\n",
    "        local_files.add(os.path.relpath(os.path.join(root, file), model_path))\n",
    "    \n",
    "# Compare\n",
    "missing_files = hub_files - local_files\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"Downloading HuggingFace model snapshot: {hf_model_id}\")\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    with TemporaryDirectory(suffix=\"model\", prefix=\"hf\", dir=\"/tmp\") as cache_dir:\n",
    "        snapshot_download(repo_id=hf_model_id, cache_dir=cache_dir, token=hf_token)\n",
    "        local_model_path = Path(cache_dir)\n",
    "        model_snapshot_path = str(list(local_model_path.glob(f\"**/snapshots/*/\"))[0])\n",
    "        print(f\"Model snapshot: {model_snapshot_path} completed\")\n",
    "        \n",
    "        print(f\"Copying model snapshot files to EFS...\")\n",
    "        for root, dirs, files in os.walk(model_snapshot_path):\n",
    "            for file in files:\n",
    "                full_path = os.path.join(root, file)\n",
    "                relative_path = f\"{full_path[len(model_snapshot_path)+1:]}\"\n",
    "                dst_path = os.path.join(model_path, relative_path)\n",
    "                dst_dir = os.path.dirname(dst_path)\n",
    "                os.makedirs(dst_dir, exist_ok=True)\n",
    "                print(f\"Copying {os.path.basename(full_path)}\")\n",
    "                shutil.copyfile(full_path, dst_path)\n",
    "\n",
    "\n",
    "os.environ['MODEL_ID']=model_path[len(home_dir):] # docker container volume mounts snapshots at /snapshots\n",
    "print(f\"MODEL_ID={os.environ['MODEL_ID']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Batch and TP Size\n",
    "\n",
    "### Batch Size\n",
    "\n",
    "#### Triton Inference Server with Python\n",
    "Set `MAX_NUM_SEQS` to maximum [dynamic batch](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/batcher.html#delayed-batching) size. This is an advanced setting, and its maximum value depends on available accelerator memory, and `max_queue_delay_microseconds` setting. The default value of `max_queue_delay_microseconds` is set to `1000`. If you increase `MAX_NUM_SEQS` above `8` you may also need to increase `max_queue_delay_microseconds`.\n",
    "\n",
    "See [triton-embeddings-cuda.sh](..//triton-server/embeddings/triton-embeddings-cuda.sh) and [tirton-embeddings-neuroxn.sh](../triton-server/embeddings/triton-embeddings-neuronx.sh) for setting `max_queue_delay_microseconds`.\n",
    "\n",
    "#### Open AI Server with VLLM\n",
    "Set `MAX_NUM_SEQS` to maximum VLLM batch size allowed by accelerator memory and model size.\n",
    "\n",
    "### TP Size\n",
    "\n",
    "For most cases, set `TENSOR_PARALLEL_SIZE` to 1 as most encoder models fit easily within 1 accelerator's memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MAX_NUM_SEQS']=str(8)\n",
    "os.environ['TENSOR_PARALLEL_SIZE']=str(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Inference Server and Engine\n",
    "\n",
    "Next, specify `inference_server`, and `inference_engine` variables, below. This notebook supports [Triton Inference Server](https://github.com/triton-inference-server/server) with `python` backend that uses custom python backend scripts. The support of Open AI server with VLLM is experimental at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_server = 'triton_inference_server'\n",
    "inference_engine = 'python'\n",
    "assert inference_server in ['triton_inference_server', 'openai_server']\n",
    "assert inference_engine in ['python', 'vllm']\n",
    "\n",
    "assert (inference_server == 'triton_inference_server' and inference_engine == 'python') \\\n",
    "    or (inference_server == 'openai_server' and inference_engine == 'vllm'), \\\n",
    "    \"triton_inference_server with python or openai_server with vlmm are supported for encoder models\"\n",
    "\n",
    "os.environ[\"INFERENCE_SERVER\"] = inference_server\n",
    "os.environ[\"INFERENCE_ENGINE\"] = inference_engine\n",
    "print(f\"Using '{inference_server}' inference server with '{inference_engine}' inference engine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Inference Server\n",
    "\n",
    "Next we use Docker compose to launch the inference server locally on this desktop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = \"./launch.sh\"\n",
    "\n",
    "! bash {script_path} down\n",
    "! bash {script_path} up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locust Testing\n",
    "\n",
    "### Load Configuration\n",
    "\n",
    "Below. we load the appropriate configuration file for the specified inference server, and backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = [ \"config\", \"encoder\", encoder_type, f\"{inference_server}-{inference_engine}.yaml\" ]\n",
    "\n",
    "config_path=os.path.join(*path)\n",
    "with open(config_path, \"r\") as mf:\n",
    "    config=yaml.safe_load(mf)\n",
    "\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify inference server is up\n",
    "\n",
    "The inference server may take several minutes to start up. Next, we wait for the inference server to start up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from endpoint_utils import wait_for_inference_ready\n",
    "wait_for_inference_ready(config['endpoint_url'], timeout_seconds=1800, interval=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Configuration\n",
    "\n",
    "The configuration file specifies a prompt generator module. The module is dynamically loaded, and is invoked iteratively by the Locust endpoint user (see `endpoint_user.py`) to get next prompt to drive Locust testing.\n",
    "\n",
    "Let us validate our configuration by making a single request and inspecting the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from endpoint_utils import inference_request\n",
    "from pprint import pprint\n",
    "import textwrap\n",
    "import json\n",
    "\n",
    "response = inference_request(config=config)\n",
    "assert int(response.status_code) == 200, f\"Response status code {response.status_code} != 200\"\n",
    "\n",
    "json_string = json.dumps(response.json())\n",
    "truncated_json_string = textwrap.shorten(json_string, width=1024, placeholder=\"...\")\n",
    "pprint(f\"Response Content: {truncated_json_string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Locust Load Testing\n",
    "\n",
    "The Locust load testing below uses 32 users with 32 workers to drive concurrent load, and by default, is set to run for 60 seconds. You can adjust these values as needed. Keep `SPAWN_RATE` the same as `USERS` to drive maximum concurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = [ \"config\", \"encoder\", encoder_type, f\"{inference_server}-{inference_engine}.yaml\" ]\n",
    "\n",
    "config_path=os.path.join(*path)\n",
    "with open(config_path, \"r\") as mf:\n",
    "    config=yaml.safe_load(mf)\n",
    "    \n",
    "ts = round(time.time() * 1000)\n",
    "\n",
    "os.environ[\"MODEL\"] = os.environ['MODEL_ID']\n",
    "os.environ[\"PROMPT_MODULE_DIR\"] = config['module_dir']\n",
    "os.environ[\"PROMPT_MODULE_NAME\"] = config['module_name']\n",
    "os.environ[\"PROMPT_GENERATOR_NAME\"] = config['prompt_generator']\n",
    "os.environ[\"TEMPLATE\"] = json.dumps(config.get('template', {}))\n",
    "os.environ[\"TEMPLATE_KEYS\"] = json.dumps(config.get('template_keys', []))\n",
    "os.environ[\"CONTENT_TYPE\"]=\"application/json\"\n",
    "os.environ[\"ENDPOINT_NAME\"] = config['endpoint_url']\n",
    "os.environ[\"USERS\"]=\"32\"\n",
    "os.environ[\"WORKERS\"]=\"32\"\n",
    "os.environ[\"RUN_TIME\"]=\"120s\"\n",
    "os.environ[\"SPAWN_RATE\"]=\"32\"\n",
    "os.environ[\"SCRIPT\"]=\"endpoint_user.py\"\n",
    "results_locust_path = os.path.join(\"output\", \"locust-testing\")\n",
    "os.environ[\"RESULTS_PREFIX\"]=f\"{results_locust_path}/results-{ts}\"\n",
    "\n",
    "try:\n",
    "    with open(\"run_locust.log\", \"w\") as logfile:\n",
    "        print(f\"Start Locust testing; logfile: run_locust.log; results: {results_locust_path}\")\n",
    "        path = os.path.join(os.getcwd(), \"run_locust.sh\")\n",
    "        os.chmod(path, stat.S_IRUSR | stat.S_IEXEC)\n",
    "        process = subprocess.Popen(path, encoding=\"utf-8\", \n",
    "                                shell=True,stdout=logfile,stderr=subprocess.STDOUT)\n",
    "        process.wait()\n",
    "        logfile.flush()\n",
    "        print(f\"Locust testing completed\")\n",
    "except Exception as e:\n",
    "    print(f\"exception occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Locust Results\n",
    "\n",
    "Below we first visualize the results of the Locust testing in a tabel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "\n",
    "results_path = os.environ[\"RESULTS_PREFIX\"] + \"_stats.csv\"\n",
    "df = pd.read_csv(results_path)\n",
    "df = df.replace(np.nan, '')\n",
    "\n",
    "top_n = 1\n",
    "caption=f\"Locust results\"\n",
    "df = df.truncate(after=top_n - 1, axis=0)\n",
    "df = df.style \\\n",
    "      .format(precision=6) \\\n",
    "        .set_properties(**{'text-align': 'left'}) \\\n",
    "        .set_caption(caption)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shutdown inference server\n",
    "\n",
    "Next, we shutdown inference server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "! bash {script_path} down"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "aws_neuron_venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
