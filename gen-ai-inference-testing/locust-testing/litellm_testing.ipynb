{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMPerf Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine requirements\n",
    "\n",
    "This notebook should be run on a machine with at least 1 Nvidia GPU, or 1 AWS AI chip. Below we show recommended machines for a range of LLM sizes. You can try smaller machines to see if they work for your tested model.\n",
    "\n",
    "**Note:** As LLM size grows bigger, it may not fit on a single machine, and cannot be tested using this notebook.\n",
    "\n",
    "### For Nvidia GPUs\n",
    "\n",
    "| LLM size      | EC2 instance type |\n",
    "| ----------- | ----------- |\n",
    "| Upto 10B      | [g6.48xlarge](https://aws.amazon.com/ec2/instance-types/g6/), [g5.48xlarge](https://aws.amazon.com/ec2/instance-types/g5/)      |\n",
    "| Upto 70B   | [g6e.48xlarge](https://aws.amazon.com/ec2/instance-types/g6e/), [p4d.24xlarge](https://aws.amazon.com/ec2/instance-types/p4/)      |\n",
    "| Upto 405B   | [p5.48xlarge](https://aws.amazon.com/ec2/instance-types/p5/), [p4de.24xlarge](https://aws.amazon.com/ec2/instance-types/p4/)      |\n",
    "| > 405B   | [p5e.48xlarge](https://aws.amazon.com/ec2/instance-types/p5/)      |\n",
    "\n",
    "### For AWS AI Chips\n",
    "\n",
    "| LLM size      | EC2 instance type |\n",
    "| ----------- | ----------- |    \n",
    "| Upto 70B   | [trn1.32xlarge](https://aws.amazon.com/ec2/instance-types/trn1/)       |\n",
    "| > 70B   | [trn2.48xlarge](https://aws.amazon.com/ec2/instance-types/trn2/)      |\n",
    "\n",
    "Next, let us verify the machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "p = subprocess.run('nvidia-smi --list-gpus | wc -l', \n",
    "                   shell=True, check=True, capture_output=True, encoding='utf-8')\n",
    "\n",
    "device = None\n",
    "num_device = 0\n",
    "\n",
    "if p.returncode == 0:\n",
    "    num_device = int(p.stdout)\n",
    "    device = \"cuda\" if num_device > 0 else None\n",
    "\n",
    "if device is None:\n",
    "    p = subprocess.run('neuron-ls -j | grep neuron_device | wc -l', \n",
    "                       shell=True, check=True, capture_output=True, encoding='utf-8')\n",
    "    if p.returncode == 0:\n",
    "        num_device = int(p.stdout)\n",
    "        device = \"neuron\" if num_device > 0 else None\n",
    "\n",
    "assert (device == \"cuda\" and num_device >= 1) or (device == \"neuron\" and num_device >= 1), \\\n",
    "    \"Machine must have 1 Nvidia CUDA devices, or 1 AWS Neuorn Devices\"\n",
    "print(f\"Auto detected {num_device} {device} devices\")\n",
    "os.environ['NUM_DEVICE']=str(num_device)\n",
    "\n",
    "if device == 'neuron':\n",
    "    p = subprocess.run(\"neuron-ls --json-output | grep nc_count | head -1 | awk -F': ' '{print $2}' | tr -d ','\",\n",
    "                       shell=True, check=True, capture_output=True, encoding='utf-8')\n",
    "    if p.returncode == 0:\n",
    "        neuron_cores_per_device = int(p.stdout)\n",
    "        print(f\"NeuronCores per device: {neuron_cores_per_device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install datasets==3.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Hugging Face User Access Token\n",
    "\n",
    "Many of the popular Large Language Models (LLMs) in Hugging Face are [gated models](https://huggingface.co/docs/hub/en/models-gated). To access gated models, you need a Hugging Face [user access token](https://huggingface.co/docs/hub/en/security-tokens). Please create a Hugging Face user access token in your Hugging Face account, and set it below in `hf_token` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "hf_token=''\n",
    "# Comment out next line if not using a Hugging Face gated model\n",
    "assert hf_token, \"Hugging Face user access token is required for gated models\"\n",
    "os.environ['HF_TOKEN']=hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Docker Containers\n",
    "\n",
    "Next, we build the docker containers used to run the inference endpoint locally on this desktop. For VLLM containers for Neuron, the default option is to install stock [VLLM](https://github.com/vllm-project/vllm). You may optionally build Neuron containers with [Neuron Upstreaming to VLLM](https://github.com/aws-neuron/upstreaming-to-vllm) by changing the following command, as shown below:\n",
    "\n",
    "        ! USE_NEURON_VLLM=true bash build-containers.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! source build-containers.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Hugging Face Model Id\n",
    "\n",
    "Next, set the Hugging Face Model Id for the LLM you want to test in `hf_model_id` variable, below. You can specify the Hugging Face model id for a [Meta Llama](https://huggingface.co/meta-llama), [Deepseek](https://huggingface.co/deepseek-ai), or [Mistral AI](https://huggingface.co/mistralai) text-based Generative AI LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_id = 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B'\n",
    "os.environ['MAX_MODEL_LEN']=str(8192)\n",
    "multi_modal_model=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snapshot Huggingface model\n",
    "\n",
    "Below we snapshot the Huggingface model and store it on the EFS. This is only done once. To force a refresh of the model from Huggingface hub, you must delete the local copy of the model from the EFS.\n",
    "\n",
    "To use EFS, we create a symbolic link from `/home/ubuntu/snapshots` to `/home/ubuntu/efs/home/snapshots` directory. Please ensure `/home/ubuntu/efs/home` exists and is owned by user `ubuntu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download, list_repo_files\n",
    "from tempfile import TemporaryDirectory\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import pwd\n",
    "\n",
    "home_dir = os.path.join(os.getenv('HOME'))\n",
    "efs_home = os.path.join(home_dir, \"efs\", \"home\")\n",
    "\n",
    "assert os.path.isdir(efs_home), f\"{efs_home} directory must exist\"\n",
    "\n",
    "stat_info = os.stat(efs_home)\n",
    "owner_uid = stat_info.st_uid\n",
    "owner_username = pwd.getpwuid(owner_uid).pw_name\n",
    "assert owner_username == \"ubuntu\", f\"{efs_home} must be owned by ubuntu\"\n",
    "efs_snapshots = os.path.join(efs_home, \"snapshots\")\n",
    "os.makedirs(efs_snapshots, exist_ok=True)\n",
    "if not os.path.exists(os.path.join(home_dir, \"snapshots\")):\n",
    "    os.symlink(efs_snapshots, os.path.join(home_dir, \"snapshots\")) # create a symbolic link to EFS directory\n",
    "\n",
    "hf_home = os.path.join(home_dir, \"snapshots\", \"huggingface\")\n",
    "os.makedirs(hf_home, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(hf_home, hf_model_id)\n",
    "\n",
    "hub_files = set(list_repo_files(repo_id=hf_model_id, repo_type=\"model\"))\n",
    "# Get local files (recursively)\n",
    "local_files = set()\n",
    "missing_files=None\n",
    "for root, dirs, files in os.walk(model_path):\n",
    "    for file in files:\n",
    "        # Get relative path from model root\n",
    "        local_files.add(os.path.relpath(os.path.join(root, file), model_path))\n",
    "    \n",
    "# Compare\n",
    "missing_files = hub_files - local_files\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"Downloading HuggingFace model snapshot: {hf_model_id}\")\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    with TemporaryDirectory(suffix=\"model\", prefix=\"hf\", dir=\"/tmp\") as cache_dir:\n",
    "        snapshot_download(repo_id=hf_model_id, cache_dir=cache_dir, token=hf_token)\n",
    "        local_model_path = Path(cache_dir)\n",
    "        model_snapshot_path = str(list(local_model_path.glob(f\"**/snapshots/*/\"))[0])\n",
    "        print(f\"Model snapshot: {model_snapshot_path} completed\")\n",
    "        \n",
    "        print(f\"Copying model snapshot files to EFS...\")\n",
    "        for root, dirs, files in os.walk(model_snapshot_path):\n",
    "            for file in files:\n",
    "                full_path = os.path.join(root, file)\n",
    "                relative_path = f\"{full_path[len(model_snapshot_path)+1:]}\"\n",
    "                dst_path = os.path.join(model_path, relative_path)\n",
    "                dst_dir = os.path.dirname(dst_path)\n",
    "                os.makedirs(dst_dir, exist_ok=True)\n",
    "                print(f\"Copying {os.path.basename(full_path)}\")\n",
    "                shutil.copyfile(full_path, dst_path)\n",
    "\n",
    "\n",
    "os.environ['MODEL_ID']=model_path[len(home_dir):] # docker container volume mounts snapshots at /snapshots\n",
    "print(f\"MODEL_ID={os.environ['MODEL_ID']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Tensor Parallel Size\n",
    "\n",
    "Tensor parallel size depends on number of available device cores (i.e. GPUs, or NeuronCores), and model size. We set tensor parallel size, by default, to minimum of number of available cores, or 8. Depending on the size of your model, you may adjust tensor parallel size , down or up, as needed. The number of inference servers is computed by dividing number of cores by tensor parallel size. For example, if there are 8 cores, and tensor parallel size is 2, 4 inference servers will be started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = num_device * neuron_cores_per_device if device == \"neuron\" else num_device\n",
    "tp_parallel_size = min(8, num_cores)\n",
    "while (num_cores % tp_parallel_size) != 0:\n",
    "    tp_parallel_size = tp_parallel_size // 2\n",
    "\n",
    "print(f\"tp_parallel_size: {tp_parallel_size}\")\n",
    "os.environ['TENSOR_PARALLEL_SIZE']=str(tp_parallel_size)\n",
    "max_num_seqs=max(tp_parallel_size, 2)\n",
    "print(f\"max_num_seqs: {max_num_seqs}\")\n",
    "os.environ['MAX_NUM_SEQS']=str(max_num_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Inference Server and Backend\n",
    "\n",
    "Next, specify `inference_server`, and `backend` variables, below. This notebook supports [Triton Inference Server](https://github.com/triton-inference-server/server) with [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), and [vLLM](https://github.com/vllm-project/vllm) as backends. It also supports Open AI compatible inference server with [vLLM](https://github.com/vllm-project/vllm) as backend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_server = 'openai_server'\n",
    "assert inference_server in ['triton_inference_server', \"openai_server\", \"djl_serving\"]\n",
    "backend = 'vllm'\n",
    "assert backend in ['vllm', 'trtllm', 'tnx']\n",
    "\n",
    "print(f\"Using '{inference_server}' inference server with '{backend}' backend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the backend is `trtllm`, we must specify the path to the [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/releases/tag/v0.21.0) Python script that converts the Hugging Face model weights to TensorRT checkpoint weights for your selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if inference_server == \"triton_inference_server\" and backend == 'trtllm':\n",
    "    # set the convert ckpt script path compatible with your model\n",
    "    os.environ[\"TRTLLM_CONVERT_CKPT_SCRIPT\"]=\"/opt/TensorRT-LLM/examples/models/core/llama/convert_checkpoint.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Inference Server\n",
    "\n",
    "Next we use Docker compose to launch the inference server locally on this desktop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_map = {\n",
    "    \"triton_inference_server\": {\n",
    "        \"vllm\": {\n",
    "            \"cuda\": \"../triton-server/vllm/compose-triton-vllm-cuda.sh\",\n",
    "            \"neuron\": \"../triton-server/vllm/compose-triton-vllm-neuronx.sh\"\n",
    "        },\n",
    "        \"trtllm\": {\n",
    "            \"cuda\": \"../triton-server/tensorrt-llm/compose-triton-tensorrt-llm.sh\"\n",
    "        },\n",
    "        \"tnx\": {\n",
    "            \"neuron\": \"../triton-server/djl-lmi/compose-triton-djl-lmi-neuronx.sh\"\n",
    "        }\n",
    "    },\n",
    "    \"openai_server\": {\n",
    "        \"vllm\": {\n",
    "            \"cuda\": \"../openai-server/vllm/compose-openai-server-vllm-cuda.sh\",\n",
    "            \"neuron\": \"../openai-server/vllm/compose-openai-server-vllm-neuronx.sh\"\n",
    "        }\n",
    "    },\n",
    "    \"djl_serving\": {\n",
    "        \"vllm\": {\n",
    "            \"cuda\": \"../djl-serving/vllm/compose-djl-lmi-vllm-cuda.sh\",\n",
    "            \"neuron\": \"../djl-serving/vllm/compose-djl-lmi-vllm-neuronx.sh\"\n",
    "        },\n",
    "        \"trtllm\": {\n",
    "            \"cuda\": \"../djl-serving/tensorrt-llm/compose-djl-lmi-tensorrt-llm.sh\"\n",
    "        },\n",
    "        \"tnx\": {\n",
    "            \"neuron\": \"../djl-serving/transformers-neuronx/compose-djl-lmi-transformers-neuronx.sh\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "try:\n",
    "    script_path = script_map[inference_server][backend][device]\n",
    "except Exception as e:\n",
    "    print(f\"combination not supported: inference server: {inference_server}, backend: {backend}, device: {device}\")\n",
    "    raise e\n",
    "\n",
    "! {script_path} down\n",
    "! {script_path} up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LiteLLM Testing\n",
    "\n",
    "### Load Configuration\n",
    "\n",
    "Below. we load the appropriate configuration file for the specified inference server, and backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = [ \"config\", f\"{inference_server}-{backend}.yaml\" ]\n",
    "if multi_modal_model:\n",
    "    path.insert(1, \"multi-modal\")\n",
    "elif \"code\" in hf_model_id:\n",
    "    path.insert(1, \"code-gen\")\n",
    "\n",
    "config_path=os.path.join(*path)\n",
    "with open(config_path, \"r\") as mf:\n",
    "    config=yaml.safe_load(mf)\n",
    "\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify inference server is up\n",
    "\n",
    "The inference server may take several minutes to start up. Next, we verify the inference server is up. Do not proceed to next cell until inference server is up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "try:\n",
    "    response = requests.get(config['endpoint_url'])\n",
    "    response_code = int(response.status_code)\n",
    "    assert (response_code == 405) or (response_code == 424), f\"Inference server is not yet up: {response_code}\"\n",
    "    print(\"Inference server is up!\")\n",
    "except:\n",
    "    print(\"Inference server is not yet up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LiteLLM config file\n",
    "\n",
    "Below we create the config file for LiteLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "litellm_config_path=\"litellm_config.yaml\"\n",
    "with open(litellm_config_path, \"r\") as mf:\n",
    "    litellm_config=yaml.safe_load(mf)\n",
    "\n",
    "litellm_config['model_list'][0]['litellm_params']['model'] = f\"custom-endpoint/{hf_model_id}\"\n",
    "litellm_config['model_list'][0]['litellm_params']['api_base'] = config['endpoint_url']\n",
    "\n",
    "litellm_config['environment_variables'][\"TEMPLATE\"] = json.dumps(config.get('template', {}))\n",
    "litellm_config['environment_variables'][\"TEMPLATE_KEYS\"] = json.dumps(config.get('template_keys', []))\n",
    "litellm_config['environment_variables'][\"CONTENT_TYPE\"]=\"application/json\"\n",
    "litellm_config['environment_variables'][\"MODEL\"]=os.getenv(\"MODEL_ID\")\n",
    "litellm_config_path = \"/tmp/litellm_config.yaml\"\n",
    "with open(litellm_config_path, \"w\") as outfile:\n",
    "    yaml.dump(litellm_config, outfile, default_flow_style=False)\n",
    "\n",
    "print(f\"Configuration saved to {litellm_config_path}\")\n",
    "print(json.dumps(litellm_config, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start LiteLLM Proxy\n",
    "\n",
    "Below we start the LiteLLM Proxy as a daemoon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Docker container as a daemon process\n",
    "process = subprocess.Popen([\n",
    "    'docker', 'run',\n",
    "    '-d',  # Run in detached/daemon mode\n",
    "    '-v', f'{litellm_config_path}:/app/config.yaml',\n",
    "    '-v', './custom_endpoint_handler.py:/app/custom_endpoint_handler.py',\n",
    "    \"--network\", \"host\",\n",
    "    '-p', '4000:4000',\n",
    "    'ghcr.io/berriai/litellm:main-stable',\n",
    "    '--config', '/app/config.yaml'\n",
    "], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# Get the container ID\n",
    "stdout, stderr = process.communicate()\n",
    "\n",
    "if process.returncode == 0:\n",
    "    container_id = stdout.strip()\n",
    "    print(f\"Container started successfully with ID: {container_id}\")\n",
    "else:\n",
    "    print(f\"Error starting container: {stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the LiteLLM Proxy endpoint\n",
    "\n",
    "Below we test the LiteLLM Proxy endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "os.environ[\"MODEL\"] = os.environ['MODEL_ID']\n",
    "url = \"http://localhost:4000/v1/chat/completions\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\": litellm_config['model_list'][0]['litellm_params']['model'],\n",
    "    \"messages\": [{\"content\": \"Explain LLM inference to a novice in generative AI.\"}]\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "# Print the response\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove LLMLite Docker container\n",
    "\n",
    "Below we remove the LLMLite Docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = subprocess.Popen([\n",
    "        'docker', 'rm', '-f', container_id  # -f forces removal (stops if running)\n",
    "    ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    \n",
    "stdout, stderr = process.communicate()\n",
    "\n",
    "if process.returncode == 0:\n",
    "    print(f\"Container {container_id} stopped and removed successfully\")\n",
    "else:\n",
    "    print(f\"Error: {stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shutdown LiteLLM proxy and inference server\n",
    "\n",
    "Next, we shutdown LiteLLM proxy and the inference server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "! {script_path} down"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
