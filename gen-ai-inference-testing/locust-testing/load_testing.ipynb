{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locust Load Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine requriements\n",
    "\n",
    "This notebook should be run on a machine with at least 8 Nvidia GPUs, or 16 AWS AI chips. Below we show recommended machines for a range of LLM sizes. \n",
    "\n",
    "**Note:** As LLM size grows bigger, it may not fit on a single machine, and cannot be tested using this notebook.\n",
    "\n",
    "### For Nvidia GPUs\n",
    "\n",
    "| LLM size      | EC2 instance type |\n",
    "| ----------- | ----------- |\n",
    "| Upto 10B      | [g6.48xlarge](https://aws.amazon.com/ec2/instance-types/g6/), [g5.48xlarge](https://aws.amazon.com/ec2/instance-types/g5/)      |\n",
    "| Upto 70B   | [g6e.48xlarge](https://aws.amazon.com/ec2/instance-types/g6e/), [p4d.24xlarge](https://aws.amazon.com/ec2/instance-types/p4/)      |\n",
    "| Upto 405B   | [p5.48xlarge](https://aws.amazon.com/ec2/instance-types/p5/), [p4de.24xlarge](https://aws.amazon.com/ec2/instance-types/p4/)      |\n",
    "| > 405B   | [p5e.48xlarge](https://aws.amazon.com/ec2/instance-types/p5/)      |\n",
    "\n",
    "### For AWS AI Chips\n",
    "\n",
    "| LLM size      | EC2 instance type |\n",
    "| ----------- | ----------- |    \n",
    "| Upto 70B   | [trn1.32xlarge](https://aws.amazon.com/ec2/instance-types/trn1/)       |\n",
    "| > 70B   | [trn2.48xlarge](https://aws.amazon.com/ec2/instance-types/trn2/)      |\n",
    "\n",
    "Next, let us verify the machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "p = subprocess.run('nvidia-smi --list-gpus | wc -l', \n",
    "                   shell=True, check=True, capture_output=True, encoding='utf-8')\n",
    "\n",
    "\n",
    "device = None\n",
    "num_device = 0\n",
    "\n",
    "if p.returncode == 0:\n",
    "    num_device = int(p.stdout)\n",
    "    device = \"cuda\" if num_device > 0 else None\n",
    "\n",
    "if device is None:\n",
    "    p = subprocess.run('neuron-ls -j | grep neuron_device | wc -l', \n",
    "                       shell=True, check=True, capture_output=True, encoding='utf-8')\n",
    "    if p.returncode == 0:\n",
    "        num_device = int(p.stdout)\n",
    "        device = \"neuron\" if num_device > 0 else None\n",
    "\n",
    "assert (device == \"cuda\" and num_device == 8) or (device == \"neuron\" and num_device == 16), \\\n",
    "    \"Machine must have 8 Nvidia GPUs, or 16 AWS AI Chips\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, install required Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install locust\n",
    "!pip install datasets\n",
    "!which locust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Hugging Face User Access Token\n",
    "\n",
    "Many of the popular Large Language Models (LLMs) in Hugging Face are [gated models](https://huggingface.co/docs/hub/en/models-gated). To access gated models, you need a Hugging Face [user access token](https://huggingface.co/docs/hub/en/security-tokens). Please create a Hugging Face user access token in your Hugging Face account, and set it below in `hf_token` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import stat\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "hf_token=''\n",
    "# Comment out next line if not using a Hugging Face gated model\n",
    "assert hf_token, \"Hugging Face user access token is required for gated models\"\n",
    "os.environ['HF_TOKEN']=hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Docker Containers\n",
    "\n",
    "Next, we build the docker containers used to run the inference endpoint locally on this desktop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! source build-containers.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Hugging Face Model Id\n",
    "\n",
    "Next, set the Hugging Face Model Id for the LLM you want to test in `hf_model_id` variable, below. You can specify the Hugging Face model id for a [Meta Llama](https://huggingface.co/meta-llama), [Deepseek](https://huggingface.co/deepseek-ai), or [Mistral AI](https://huggingface.co/mistralai) text-based Generative AI LLM. Multi-modal LLMs are not yet supported in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_id = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "os.environ['MODEL_ID']=hf_model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Inference Server and Backend\n",
    "\n",
    "Next, specify `inference_server`, and `backend` variables, below. This notebook supports [Triton Inference Server](https://github.com/triton-inference-server/server) with [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), [vLLM](https://github.com/vllm-project/vllm), and [Transformers Neuronx in LMI engines](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/user_guides/tnx_user_guide.html) as backends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_server = 'triton_inference_server'\n",
    "assert inference_server in ['triton_inference_server']\n",
    "backend = 'vllm'\n",
    "assert backend in ['vllm', 'trtllm', 'tnx_lmi']\n",
    "\n",
    "assert backend != 'trtllm' or device == \"cuda\", f\"TesnorRT-LLM not supported on {device} device\"\n",
    "assert backend != 'tnx_lmi' or device == \"neuron\", f\"Transformers-Neuronx in LMI is not supported on {device} device\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Inference Server\n",
    "\n",
    "Next we use Docker compose to launch the inference server locally on this desktop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_map = {\n",
    "    \"triton_inference_server\": {\n",
    "        \"vllm\": {\n",
    "            \"cuda\": \"../triton-server/vllm/compose-triton-vllm-cuda.sh\",\n",
    "            \"neuron\": \"../triton-server/vllm/compose-triton-vllm-neuronx.sh\"\n",
    "        },\n",
    "        \"trtllm\": {\n",
    "            \"cuda\": \"../triton-server/tensorrt-llm/compose-triton-tensorrt-llm.sh\"\n",
    "        },\n",
    "        \"tnx_lmi\": {\n",
    "            \"neuron\": \"../triton-server/djl-lmi/compose-triton-djl-lmi-neuronx.sh\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "script_path = script_map[inference_server][backend][device]\n",
    "! {script_path} down\n",
    "! {script_path} up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locust Testing\n",
    "\n",
    "### Load Configuration\n",
    "\n",
    "Below. we load the appropriate configuration file for the specified inference server, and backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path=f\"config/{inference_server}-{backend}.yaml\"\n",
    "with open(config_path, \"r\") as mf:\n",
    "    config=yaml.safe_load(mf)\n",
    "\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify inference server is up\n",
    "\n",
    "The inference server may take several minutes to start up. Next, we verify the inference server is up. Do not proceed to next cell until inference server is up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "try:\n",
    "    response = requests.get(config['endpoint_url'])\n",
    "    assert int(response.status_code) == 405, \"Inference server is not yet up\"\n",
    "    print(\"Inference server is up!\")\n",
    "except:\n",
    "    print(\"Inference server is not yet up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Configuration\n",
    "\n",
    "The configuration file specifies a prompt generator module. The module is dynamically loaded, and is invoked iteratively by the Locust endpoint user (see `endpoint_user.py`) to get next prompt to drive Locust testing.\n",
    "\n",
    "Let us validate our configuration by making a single request and inspecting the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import import_module\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "def get_prompt_generator():\n",
    "    prompt_module_dir = config['module_dir']\n",
    "    sys.path.append(prompt_module_dir)\n",
    "    \n",
    "    prompt_module_name = config['module_name']\n",
    "    prompt_module=import_module(prompt_module_name)\n",
    "    \n",
    "    prompt_generator_name = config['prompt_generator']\n",
    "    prompt_generator_class = getattr(prompt_module, prompt_generator_name)\n",
    "\n",
    "    kwargs = config.get('prompt_kwargs', {})\n",
    "    return prompt_generator_class()(), kwargs\n",
    "      \n",
    "def inference_request():\n",
    "    prompt_generator, kwargs = get_prompt_generator()\n",
    "    text_input = next(prompt_generator)\n",
    "    data= { \"text_input\": text_input }\n",
    "    data.update(kwargs)\n",
    "    \n",
    "    body = json.dumps(data).encode(\"utf-8\")\n",
    "    pprint(body)\n",
    "    headers = {\"Content-Type\":  \"application/json\"}\n",
    "    response = requests.post(config['endpoint_url'], data=body, headers=headers)\n",
    "    return response\n",
    "\n",
    "response = inference_request()\n",
    "assert int(response.status_code) == 200, f\"Response status code {response.status_code} != 200\"\n",
    "pprint(f\"Response Content: {response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Locust Load Testing\n",
    "\n",
    "The Locust load testing below uses 32 users with 32 workers to drive concurrent load, and by default, is set to run for 60 seconds. You can adjust these values as needed. Keep `SPAWN_RATE` the same as `USERS` to drive maximum concurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ts = round(time.time() * 1000)\n",
    "\n",
    "os.environ[\"PROMPT_MODULE_DIR\"] = config['module_dir']\n",
    "os.environ[\"PROMPT_MODULE_NAME\"] = config['module_name']\n",
    "os.environ[\"PROMPT_GENERATOR_NAME\"] = config['prompt_generator']\n",
    "os.environ[\"PROMPT_KWARGS\"] = json.dumps(config.get('prompt_kwargs', {}))\n",
    "os.environ[\"CONTENT_TYPE\"]=\"application/json\"\n",
    "os.environ[\"ENDPOINT_NAME\"] = config['endpoint_url']\n",
    "os.environ[\"USERS\"]=\"32\"\n",
    "os.environ[\"WORKERS\"]=\"32\"\n",
    "os.environ[\"RUN_TIME\"]=\"60s\"\n",
    "os.environ[\"SPAWN_RATE\"]=\"32\"\n",
    "os.environ[\"SCRIPT\"]=\"endpoint_user.py\"\n",
    "results_locust_path = os.path.join(\"output\", \"locust-testing\")\n",
    "os.environ[\"RESULTS_PREFIX\"]=f\"{results_locust_path}/results-{ts}\"\n",
    "    \n",
    "try:\n",
    "    with open(\"run_locust.log\", \"w\") as logfile:\n",
    "        print(f\"Start Locust testing; logfile: run_locust.log; results: {results_locust_path}\")\n",
    "        path = os.path.join(os.getcwd(), \"run_locust.sh\")\n",
    "        os.chmod(path, stat.S_IRUSR | stat.S_IEXEC)\n",
    "        process = subprocess.Popen(path, encoding=\"utf-8\", \n",
    "                                shell=True,stdout=logfile,stderr=subprocess.STDOUT)\n",
    "        process.wait()\n",
    "        logfile.flush()\n",
    "        print(f\"Locust testing completed\")\n",
    "except Exception as e:\n",
    "    print(f\"exception occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Locust Results\n",
    "\n",
    "Below we first visualize the results of the Locust testing in a tabel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "\n",
    "results_path = os.environ[\"RESULTS_PREFIX\"] + \"_stats.csv\"\n",
    "df = pd.read_csv(results_path)\n",
    "df = df.replace(np.nan, '')\n",
    "\n",
    "top_n = 1\n",
    "caption=f\"Locust results\"\n",
    "df = df.truncate(after=top_n - 1, axis=0)\n",
    "df = df.style \\\n",
    "      .format(precision=6) \\\n",
    "        .set_properties(**{'text-align': 'left'}) \\\n",
    "        .set_caption(caption)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shutdown inference server\n",
    "\n",
    "Next, we shutdown inference server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! {script_path} down"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
