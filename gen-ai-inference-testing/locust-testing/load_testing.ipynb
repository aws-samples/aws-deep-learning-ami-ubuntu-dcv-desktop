{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locust Load Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine requriements\n",
    "\n",
    "This notebook should be run on a machine with at least 8 Nvidia GPUs, or 16 AWS AI chips. Below we show recommended machines for a range of LLM sizes. \n",
    "\n",
    "**Note:** As LLM size grows bigger, it may not fit on a single machine, and cannot be tested using this notebook.\n",
    "\n",
    "### For Nvidia GPUs\n",
    "\n",
    "| LLM size      | EC2 instance type |\n",
    "| ----------- | ----------- |\n",
    "| Upto 10B      | [g6.48xlarge](https://aws.amazon.com/ec2/instance-types/g6/), [g5.48xlarge](https://aws.amazon.com/ec2/instance-types/g5/)      |\n",
    "| Upto 70B   | [g6e.48xlarge](https://aws.amazon.com/ec2/instance-types/g6e/), [p4d.24xlarge](https://aws.amazon.com/ec2/instance-types/p4/)      |\n",
    "| Upto 405B   | [p5.48xlarge](https://aws.amazon.com/ec2/instance-types/p5/), [p4de.24xlarge](https://aws.amazon.com/ec2/instance-types/p4/)      |\n",
    "| > 405B   | [p5e.48xlarge](https://aws.amazon.com/ec2/instance-types/p5/)      |\n",
    "\n",
    "### For AWS AI Chips\n",
    "\n",
    "| LLM size      | EC2 instance type |\n",
    "| ----------- | ----------- |    \n",
    "| Upto 70B   | [trn1.32xlarge](https://aws.amazon.com/ec2/instance-types/trn1/)       |\n",
    "| > 70B   | [trn2.48xlarge](https://aws.amazon.com/ec2/instance-types/trn2/)      |\n",
    "\n",
    "Next, let us verify the machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "p = subprocess.run('nvidia-smi --list-gpus | wc -l', \n",
    "                   shell=True, check=True, capture_output=True, encoding='utf-8')\n",
    "\n",
    "device = None\n",
    "num_device = 0\n",
    "\n",
    "if p.returncode == 0:\n",
    "    num_device = int(p.stdout)\n",
    "    device = \"cuda\" if num_device > 0 else None\n",
    "\n",
    "if device is None:\n",
    "    p = subprocess.run('neuron-ls -j | grep neuron_device | wc -l', \n",
    "                       shell=True, check=True, capture_output=True, encoding='utf-8')\n",
    "    if p.returncode == 0:\n",
    "        num_device = int(p.stdout)\n",
    "        device = \"neuron\" if num_device > 0 else None\n",
    "\n",
    "assert (device == \"cuda\" and num_device == 8) or (device == \"neuron\" and num_device == 16), \\\n",
    "    \"Machine must have 8 Nvidia CUDA devices, or 16 AWS Neuorn Devices\"\n",
    "print(f\"Auto detected {num_device} {device} devices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, install required Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install locust\n",
    "!pip install datasets\n",
    "!which locust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Hugging Face User Access Token\n",
    "\n",
    "Many of the popular Large Language Models (LLMs) in Hugging Face are [gated models](https://huggingface.co/docs/hub/en/models-gated). To access gated models, you need a Hugging Face [user access token](https://huggingface.co/docs/hub/en/security-tokens). Please create a Hugging Face user access token in your Hugging Face account, and set it below in `hf_token` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import stat\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "hf_token=''\n",
    "# Comment out next line if not using a Hugging Face gated model\n",
    "assert hf_token, \"Hugging Face user access token is required for gated models\"\n",
    "os.environ['HF_TOKEN']=hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Docker Containers\n",
    "\n",
    "Next, we build the docker containers used to run the inference endpoint locally on this desktop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! source build-containers.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Hugging Face Model Id\n",
    "\n",
    "Next, set the Hugging Face Model Id for the LLM you want to test in `hf_model_id` variable, below. You can specify the Hugging Face model id for a [Meta Llama](https://huggingface.co/meta-llama), [Deepseek](https://huggingface.co/deepseek-ai), or [Mistral AI](https://huggingface.co/mistralai) text-based Generative AI LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_id = 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B'\n",
    "os.environ['MODEL_ID']=hf_model_id\n",
    "os.environ['MAX_MODEL_LEN']=str(8192)\n",
    "multi_modal_model=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Tensor Parallel Size\n",
    "\n",
    "Tensor parallel size depends on number of available device cores, and model size. We set tensor parallel size, by default, to minimum of number of available cores, or 8. Depeneding on the size of your model, you may adjust tensor parallel size , down or up, as needed. The number of backend inference servers is computed by dividing number of devices by tensor parallel size. For example, if there are 8 devices, and tensor parallel size is 2, 4 backend inference servers will be started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TENSOR_PARALLEL_SIZE']=str(min(8, num_device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Inference Server and Backend\n",
    "\n",
    "Next, specify `inference_server`, and `backend` variables, below. This notebook supports [Triton Inference Server](https://github.com/triton-inference-server/server) with [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), [vLLM](https://github.com/vllm-project/vllm), and [Transformers Neuronx in LMI engines](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/user_guides/tnx_user_guide.html) as backends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_server = 'triton_inference_server'\n",
    "assert inference_server in ['triton_inference_server', \"djl_serving\", \"openai_server\"]\n",
    "backend = 'vllm'\n",
    "assert backend in ['vllm', 'trtllm', 'tnx_lmi']\n",
    "\n",
    "assert not multi_modal_model or (inference_server == \"openai_server\" and device==\"cuda\"), \"Multi modal requires openai_server and cuda\"\n",
    "assert inference_server != \"openai_server\" or backend == \"vllm\", f\"openai_server requires vllm backend\"\n",
    "assert backend != 'trtllm' or device == \"cuda\", f\"TensorRT-LLM not supported on {device} device\"\n",
    "assert backend != 'tnx_lmi' or device == \"neuron\", f\"Transformers-Neuronx-LMI is not supported on {device} device\"\n",
    "\n",
    "print(f\"Using '{inference_server}' inference server with '{backend}' backend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Inference Server\n",
    "\n",
    "Next we use Docker compose to launch the inference server locally on this desktop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_map = {\n",
    "    \"triton_inference_server\": {\n",
    "        \"vllm\": {\n",
    "            \"cuda\": \"../triton-server/vllm/compose-triton-vllm-cuda.sh\",\n",
    "            \"neuron\": \"../triton-server/vllm/compose-triton-vllm-neuronx.sh\"\n",
    "        },\n",
    "        \"trtllm\": {\n",
    "            \"cuda\": \"../triton-server/tensorrt-llm/compose-triton-tensorrt-llm.sh\"\n",
    "        },\n",
    "        \"tnx_lmi\": {\n",
    "            \"neuron\": \"../triton-server/djl-lmi/compose-triton-djl-lmi-neuronx.sh\"\n",
    "        }\n",
    "    },\n",
    "    \"djl_serving\": {\n",
    "        \"vllm\": {\n",
    "            \"cuda\": \"../djl-serving/vllm/compose-djl-lmi-vllm-cuda.sh\",\n",
    "            \"neuron\": \"../djl-serving/vllm/compose-djl-lmi-vllm-neuronx.sh\"\n",
    "        },\n",
    "        \"trtllm\": {\n",
    "            \"cuda\": \"../djl-serving/tensorrt-llm/compose-djl-lmi-tensorrt-llm.sh\"\n",
    "        },\n",
    "        \"tnx_lmi\": {\n",
    "            \"neuron\": \"../djl-serving/transformers-neuronx/compose-djl-lmi-transformers-neuronx.sh\"\n",
    "        }\n",
    "    },\n",
    "    \"openai_server\": {\n",
    "        \"vllm\": {\n",
    "            \"cuda\": \"../openai-server/vllm/compose-openai-server-vllm-cuda.sh\",\n",
    "            \"neuron\": \"../openai-server/vllm/compose-openai-server-vllm-neuronx.sh\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "script_path = script_map[inference_server][backend][device]\n",
    "! {script_path} down\n",
    "! {script_path} up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locust Testing\n",
    "\n",
    "### Load Configuration\n",
    "\n",
    "Below. we load the appropriate configuration file for the specified inference server, and backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = [ \"config\", f\"{inference_server}-{backend}.yaml\" ]\n",
    "if multi_modal_model:\n",
    "    path.insert(1, \"multi-modal\")\n",
    "\n",
    "config_path=os.path.join(*path)\n",
    "with open(config_path, \"r\") as mf:\n",
    "    config=yaml.safe_load(mf)\n",
    "\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify inference server is up\n",
    "\n",
    "The inference server may take several minutes to start up. Next, we verify the inference server is up. Do not proceed to next cell until inference server is up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "try:\n",
    "    response = requests.get(config['endpoint_url'])\n",
    "    response_code = int(response.status_code)\n",
    "    assert (response_code == 405) or (response_code == 424), f\"Inference server is not yet up: {response_code}\"\n",
    "    print(\"Inference server is up!\")\n",
    "except:\n",
    "    print(\"Inference server is not yet up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Configuration\n",
    "\n",
    "The configuration file specifies a prompt generator module. The module is dynamically loaded, and is invoked iteratively by the Locust endpoint user (see `endpoint_user.py`) to get next prompt to drive Locust testing.\n",
    "\n",
    "Let us validate our configuration by making a single request and inspecting the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import import_module\n",
    "import re\n",
    "from pprint import pprint\n",
    "import sys\n",
    "\n",
    "def get_prompt_generator():\n",
    "    prompt_module_dir = config['module_dir']\n",
    "    sys.path.append(prompt_module_dir)\n",
    "    \n",
    "    prompt_module_name = config['module_name']\n",
    "    prompt_module=import_module(prompt_module_name)\n",
    "    \n",
    "    prompt_generator_name = config['prompt_generator']\n",
    "    prompt_generator_class = getattr(prompt_module, prompt_generator_name)\n",
    "\n",
    "    return prompt_generator_class()()\n",
    "      \n",
    "def fill_template(template: dict, template_keys:list, inputs:list) -> dict:\n",
    "        \n",
    "    assert len(template_keys) == len(inputs), f\"template_keys: {template_keys}, prompts: {inputs}\"\n",
    "    for i, template_key in enumerate(template_keys):\n",
    "        _template = template\n",
    "        keys = template_key.split(\".\")\n",
    "        for key in keys[:-1]:\n",
    "            m = re.match(r'\\[(\\d+)\\]', key)\n",
    "            if m:\n",
    "                key = int(m.group(1))\n",
    "            _template = _template[key]\n",
    "\n",
    "        _template[keys[-1]] = inputs[i]\n",
    "    \n",
    "    return template\n",
    "\n",
    "def inference_request():\n",
    "    prompt_generator = get_prompt_generator()\n",
    "    inputs = next(prompt_generator)\n",
    "    inputs = [inputs] if isinstance(inputs, str) else inputs\n",
    "\n",
    "    template = config['template']\n",
    "    assert template is not None\n",
    "\n",
    "    template_keys = config['template_keys']\n",
    "    assert template_keys is not None\n",
    "    \n",
    "    if \"model\" in template_keys:\n",
    "        inputs.insert(0, hf_model_id)\n",
    "    data = fill_template(template=template, template_keys=template_keys, inputs=inputs)\n",
    "\n",
    "    body = json.dumps(data).encode(\"utf-8\")\n",
    "    pprint(body)\n",
    "    headers = {\"Content-Type\":  \"application/json\"}\n",
    "    response = requests.post(config['endpoint_url'], data=body, headers=headers)\n",
    "    return response\n",
    "\n",
    "response = inference_request()\n",
    "assert int(response.status_code) == 200, f\"Response status code {response.status_code} != 200\"\n",
    "pprint(f\"Response Content: {response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Locust Load Testing\n",
    "\n",
    "The Locust load testing below uses 32 users with 32 workers to drive concurrent load, and by default, is set to run for 60 seconds. You can adjust these values as needed. Keep `SPAWN_RATE` the same as `USERS` to drive maximum concurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ts = round(time.time() * 1000)\n",
    "\n",
    "os.environ[\"MODEL\"] = hf_model_id\n",
    "os.environ[\"PROMPT_MODULE_DIR\"] = config['module_dir']\n",
    "os.environ[\"PROMPT_MODULE_NAME\"] = config['module_name']\n",
    "os.environ[\"PROMPT_GENERATOR_NAME\"] = config['prompt_generator']\n",
    "os.environ[\"TEMPLATE\"] = json.dumps(config.get('template', {}))\n",
    "os.environ[\"TEMPLATE_KEYS\"] = json.dumps(config.get('template_keys', []))\n",
    "os.environ[\"CONTENT_TYPE\"]=\"application/json\"\n",
    "os.environ[\"ENDPOINT_NAME\"] = config['endpoint_url']\n",
    "os.environ[\"USERS\"]=\"32\"\n",
    "os.environ[\"WORKERS\"]=\"32\"\n",
    "os.environ[\"RUN_TIME\"]=\"120s\"\n",
    "os.environ[\"SPAWN_RATE\"]=\"32\"\n",
    "os.environ[\"SCRIPT\"]=\"endpoint_user.py\"\n",
    "results_locust_path = os.path.join(\"output\", \"locust-testing\")\n",
    "os.environ[\"RESULTS_PREFIX\"]=f\"{results_locust_path}/results-{ts}\"\n",
    "    \n",
    "try:\n",
    "    with open(\"run_locust.log\", \"w\") as logfile:\n",
    "        print(f\"Start Locust testing; logfile: run_locust.log; results: {results_locust_path}\")\n",
    "        path = os.path.join(os.getcwd(), \"run_locust.sh\")\n",
    "        os.chmod(path, stat.S_IRUSR | stat.S_IEXEC)\n",
    "        process = subprocess.Popen(path, encoding=\"utf-8\", \n",
    "                                shell=True,stdout=logfile,stderr=subprocess.STDOUT)\n",
    "        process.wait()\n",
    "        logfile.flush()\n",
    "        print(f\"Locust testing completed\")\n",
    "except Exception as e:\n",
    "    print(f\"exception occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Locust Results\n",
    "\n",
    "Below we first visualize the results of the Locust testing in a tabel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "\n",
    "results_path = os.environ[\"RESULTS_PREFIX\"] + \"_stats.csv\"\n",
    "df = pd.read_csv(results_path)\n",
    "df = df.replace(np.nan, '')\n",
    "\n",
    "top_n = 1\n",
    "caption=f\"Locust results\"\n",
    "df = df.truncate(after=top_n - 1, axis=0)\n",
    "df = df.style \\\n",
    "      .format(precision=6) \\\n",
    "        .set_properties(**{'text-align': 'left'}) \\\n",
    "        .set_caption(caption)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shutdown inference server\n",
    "\n",
    "Next, we shutdown inference server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! {script_path} down"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "aws_neuron_venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
