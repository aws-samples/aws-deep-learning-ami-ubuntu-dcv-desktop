{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locust Load Testing for Embeddings Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine requirements\n",
    "\n",
    "This notebook should be run on a machine with at least 1 Nvidia GPU, or 1 AWS AI chip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "p = subprocess.run('nvidia-smi --list-gpus | wc -l', \n",
    "                   shell=True, check=True, capture_output=True, encoding='utf-8')\n",
    "\n",
    "device = None\n",
    "num_device = 0\n",
    "\n",
    "if p.returncode == 0:\n",
    "    num_device = int(p.stdout)\n",
    "    device = \"cuda\" if num_device > 0 else None\n",
    "\n",
    "if device is None:\n",
    "    p = subprocess.run('neuron-ls -j | grep neuron_device | wc -l', \n",
    "                       shell=True, check=True, capture_output=True, encoding='utf-8')\n",
    "    if p.returncode == 0:\n",
    "        num_device = int(p.stdout)\n",
    "        device = \"neuron\" if num_device > 0 else None\n",
    "\n",
    "assert (device == \"cuda\" and num_device >= 1) or (device == \"neuron\" and num_device >= 1), \\\n",
    "    \"Machine must have 1 Nvidia CUDA devices, or 1 AWS Neuorn Devices\"\n",
    "print(f\"Auto detected {num_device} {device} devices\")\n",
    "os.environ['NUM_DEVICE']=str(num_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, install required Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install locust\n",
    "!pip install datasets\n",
    "!which locust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Hugging Face User Access Token\n",
    "\n",
    "Many of the popular Large Language Models (LLMs) in Hugging Face are [gated models](https://huggingface.co/docs/hub/en/models-gated). To access gated models, you need a Hugging Face [user access token](https://huggingface.co/docs/hub/en/security-tokens). Please create a Hugging Face user access token in your Hugging Face account, and set it below in `hf_token` variable below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import stat\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "hf_token=''\n",
    "# Comment out next line if not using a Hugging Face gated model\n",
    "assert hf_token, \"Hugging Face user access token is required for gated models\"\n",
    "os.environ['HF_TOKEN']=hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Docker Containers\n",
    "\n",
    "Next, we build the docker containers used to run the inference endpoint locally on this desktop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! source build-containers-embeddings.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Hugging Face Model Id\n",
    "\n",
    "Next, set the Hugging Face Model Id for the embeddings model you want to test in `hf_model_id` variable, below. \n",
    "\n",
    "The variable `MAX_MODEL_LEN` should be set to the *minimum* of the maximum context length allowed by the model, and the maximum context length you want to use for your testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_id = 'BAAI/bge-large-en-v1.5'\n",
    "os.environ['MAX_MODEL_LEN']=str(512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snapshot Huggingface model\n",
    "\n",
    "Below we snapshot the Huggingface model and store it on the EFS. This is only done once. To force a refresh of the model from Huggingface hub, you must delete the local copy of the model from the EFS.\n",
    "\n",
    "To use EFS, we create a symbolic link from `/home/ubuntu/snapshots` to `/home/ubuntu/efs/home/snapshots` directory. Please ensure `/home/ubuntu/efs/home` exists and is owned by user `ubuntu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download, list_repo_files\n",
    "from tempfile import TemporaryDirectory\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import pwd\n",
    "\n",
    "home_dir = os.path.join(os.getenv('HOME'))\n",
    "efs_home = os.path.join(home_dir, \"efs\", \"home\")\n",
    "\n",
    "assert os.path.isdir(efs_home), f\"{efs_home} directory must exist\"\n",
    "\n",
    "stat_info = os.stat(efs_home)\n",
    "owner_uid = stat_info.st_uid\n",
    "owner_username = pwd.getpwuid(owner_uid).pw_name\n",
    "assert owner_username == \"ubuntu\", f\"{efs_home} must be owned by ubuntu\"\n",
    "efs_snapshots = os.path.join(efs_home, \"snapshots\")\n",
    "os.makedirs(efs_snapshots, exist_ok=True)\n",
    "if not os.path.exists(os.path.join(home_dir, \"snapshots\")):\n",
    "    os.symlink(efs_snapshots, os.path.join(home_dir, \"snapshots\")) # create a symbolic link to EFS directory\n",
    "\n",
    "hf_home = os.path.join(home_dir, \"snapshots\", \"huggingface\")\n",
    "os.makedirs(hf_home, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(hf_home, hf_model_id)\n",
    "\n",
    "hub_files = set(list_repo_files(repo_id=hf_model_id, repo_type=\"model\"))\n",
    "# Get local files (recursively)\n",
    "local_files = set()\n",
    "missing_files=None\n",
    "for root, dirs, files in os.walk(model_path):\n",
    "    for file in files:\n",
    "        # Get relative path from model root\n",
    "        local_files.add(os.path.relpath(os.path.join(root, file), model_path))\n",
    "    \n",
    "# Compare\n",
    "missing_files = hub_files - local_files\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"Downloading HuggingFace model snapshot: {hf_model_id}\")\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    with TemporaryDirectory(suffix=\"model\", prefix=\"hf\", dir=\"/tmp\") as cache_dir:\n",
    "        snapshot_download(repo_id=hf_model_id, cache_dir=cache_dir, token=hf_token)\n",
    "        local_model_path = Path(cache_dir)\n",
    "        model_snapshot_path = str(list(local_model_path.glob(f\"**/snapshots/*/\"))[0])\n",
    "        print(f\"Model snapshot: {model_snapshot_path} completed\")\n",
    "        \n",
    "        print(f\"Copying model snapshot files to EFS...\")\n",
    "        for root, dirs, files in os.walk(model_snapshot_path):\n",
    "            for file in files:\n",
    "                full_path = os.path.join(root, file)\n",
    "                relative_path = f\"{full_path[len(model_snapshot_path)+1:]}\"\n",
    "                dst_path = os.path.join(model_path, relative_path)\n",
    "                dst_dir = os.path.dirname(dst_path)\n",
    "                os.makedirs(dst_dir, exist_ok=True)\n",
    "                print(f\"Copying {os.path.basename(full_path)}\")\n",
    "                shutil.copyfile(full_path, dst_path)\n",
    "\n",
    "\n",
    "os.environ['MODEL_ID']=model_path[len(home_dir):] # docker container volume mounts snapshots at /snapshots\n",
    "print(f\"MODEL_ID={os.environ['MODEL_ID']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Dynamic Batch Size\n",
    "\n",
    "Set `MAX_NUM_SEQS` to maximum [dynamic batch](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/batcher.html#delayed-batching) size. This is an advanced setting, and its maximum value depends on available accelerator memory, and `max_queue_delay_microseconds` setting. The default value of `max_queue_delay_microseconds` is set to `1000`. If you increase `MAX_NUM_SEQS` above `8` you may also need to increase `max_queue_delay_microseconds`.\n",
    "\n",
    "See [triton-embeddings-cuda.sh](..//triton-server/embeddings/triton-embeddings-cuda.sh) and [tirton-embeddings-neuroxn.sh](../triton-server/embeddings/triton-embeddings-neuronx.sh) for setting `max_queue_delay_microseconds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MAX_NUM_SEQS']=str(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Inference Server and Backend\n",
    "\n",
    "Next, specify `inference_server`, and `backend` variables, below. This notebook supports [Triton Inference Server](https://github.com/triton-inference-server/server) with `embeddings` backend that uses a [custom python](../triton-server/embeddings/triton_embeddings_backend.py) backend for Triton Inference Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_server = 'triton_inference_server'\n",
    "backend = 'embeddings'\n",
    "\n",
    "print(f\"Using '{inference_server}' inference server with '{backend}' backend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Inference Server\n",
    "\n",
    "Next we use Docker compose to launch the inference server locally on this desktop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_map = {\n",
    "    \"triton_inference_server\": {\n",
    "        \"embeddings\": {\n",
    "            \"cuda\": \"../triton-server/embeddings/compose-triton-embeddings-cuda.sh\",\n",
    "            \"neuron\": \"../triton-server/embeddings/compose-triton-embeddings-neuronx.sh\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "script_path = script_map[inference_server][backend][device]\n",
    "! {script_path} down\n",
    "! {script_path} up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locust Testing\n",
    "\n",
    "### Load Configuration\n",
    "\n",
    "Below. we load the appropriate configuration file for the specified inference server, and backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = [ \"config\", f\"{inference_server}-{backend}.yaml\" ]\n",
    "\n",
    "config_path=os.path.join(*path)\n",
    "with open(config_path, \"r\") as mf:\n",
    "    config=yaml.safe_load(mf)\n",
    "\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify inference server is up\n",
    "\n",
    "The inference server may take several minutes to start up. Next, we verify the inference server is up. Do not proceed to next cell until inference server is up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "try:\n",
    "    response = requests.get(config['endpoint_url'])\n",
    "    response_code = int(response.status_code)\n",
    "    assert (response_code == 405) or (response_code == 424), f\"Inference server is not yet up: {response_code}\"\n",
    "    print(\"Inference server is up!\")\n",
    "except:\n",
    "    print(\"Inference server is not yet up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Configuration\n",
    "\n",
    "The configuration file specifies a prompt generator module. The module is dynamically loaded, and is invoked iteratively by the Locust endpoint user (see `endpoint_user.py`) to get next prompt to drive Locust testing.\n",
    "\n",
    "Let us validate our configuration by making a single request and inspecting the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import import_module\n",
    "import re\n",
    "from pprint import pprint\n",
    "import sys\n",
    "\n",
    "def get_prompt_generator():\n",
    "    prompt_module_dir = config['module_dir']\n",
    "    sys.path.append(prompt_module_dir)\n",
    "    \n",
    "    prompt_module_name = config['module_name']\n",
    "    prompt_module=import_module(prompt_module_name)\n",
    "    \n",
    "    prompt_generator_name = config['prompt_generator']\n",
    "    prompt_generator_class = getattr(prompt_module, prompt_generator_name)\n",
    "\n",
    "    return prompt_generator_class()()\n",
    "      \n",
    "def fill_template(template: dict, template_keys:list, inputs:list) -> dict:\n",
    "        \n",
    "    assert len(template_keys) == len(inputs), f\"template_keys: {template_keys}, prompts: {inputs}\"\n",
    "    for i, template_key in enumerate(template_keys):\n",
    "        _template = template\n",
    "        keys = template_key.split(\".\")\n",
    "        for key in keys[:-1]:\n",
    "            m = re.match(r'\\[(\\d+)\\]', key)\n",
    "            if m:\n",
    "                key = int(m.group(1))\n",
    "            _template = _template[key]\n",
    "\n",
    "        _template[keys[-1]] = inputs[i]\n",
    "    \n",
    "    return template\n",
    "\n",
    "def inference_request():\n",
    "    prompt_generator = get_prompt_generator()\n",
    "    inputs = next(prompt_generator)\n",
    "    inputs = [inputs] if isinstance(inputs, str) else inputs\n",
    "\n",
    "    template = config['template']\n",
    "    assert template is not None\n",
    "\n",
    "    template_keys = config['template_keys']\n",
    "    assert template_keys is not None\n",
    "    \n",
    "    if \"model\" in template_keys:\n",
    "        inputs.insert(0, os.environ['MODEL_ID'])\n",
    "    data = fill_template(template=template, template_keys=template_keys, inputs=inputs)\n",
    "\n",
    "    body = json.dumps(data).encode(\"utf-8\")\n",
    "    pprint(body)\n",
    "    headers = {\"Content-Type\":  \"application/json\"}\n",
    "    response = requests.post(config['endpoint_url'], data=body, headers=headers)\n",
    "    return response\n",
    "\n",
    "response = inference_request()\n",
    "assert int(response.status_code) == 200, f\"Response status code {response.status_code} != 200\"\n",
    "pprint(f\"Endpoint validation successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Locust Load Testing\n",
    "\n",
    "The Locust load testing below uses 32 users with 32 workers to drive concurrent load, and by default, is set to run for 60 seconds. You can adjust these values as needed. Keep `SPAWN_RATE` the same as `USERS` to drive maximum concurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = [ \"config\", f\"{inference_server}-{backend}.yaml\" ]\n",
    "\n",
    "config_path=os.path.join(*path)\n",
    "with open(config_path, \"r\") as mf:\n",
    "    config=yaml.safe_load(mf)\n",
    "    \n",
    "ts = round(time.time() * 1000)\n",
    "\n",
    "os.environ[\"MODEL\"] = os.environ['MODEL_ID']\n",
    "os.environ[\"PROMPT_MODULE_DIR\"] = config['module_dir']\n",
    "os.environ[\"PROMPT_MODULE_NAME\"] = config['module_name']\n",
    "os.environ[\"PROMPT_GENERATOR_NAME\"] = config['prompt_generator']\n",
    "os.environ[\"TEMPLATE\"] = json.dumps(config.get('template', {}))\n",
    "os.environ[\"TEMPLATE_KEYS\"] = json.dumps(config.get('template_keys', []))\n",
    "os.environ[\"CONTENT_TYPE\"]=\"application/json\"\n",
    "os.environ[\"ENDPOINT_NAME\"] = config['endpoint_url']\n",
    "os.environ[\"USERS\"]=\"32\"\n",
    "os.environ[\"WORKERS\"]=\"32\"\n",
    "os.environ[\"RUN_TIME\"]=\"120s\"\n",
    "os.environ[\"SPAWN_RATE\"]=\"32\"\n",
    "os.environ[\"SCRIPT\"]=\"endpoint_user.py\"\n",
    "results_locust_path = os.path.join(\"output\", \"locust-testing\")\n",
    "os.environ[\"RESULTS_PREFIX\"]=f\"{results_locust_path}/results-{ts}\"\n",
    "\n",
    "try:\n",
    "    with open(\"run_locust.log\", \"w\") as logfile:\n",
    "        print(f\"Start Locust testing; logfile: run_locust.log; results: {results_locust_path}\")\n",
    "        path = os.path.join(os.getcwd(), \"run_locust.sh\")\n",
    "        os.chmod(path, stat.S_IRUSR | stat.S_IEXEC)\n",
    "        process = subprocess.Popen(path, encoding=\"utf-8\", \n",
    "                                shell=True,stdout=logfile,stderr=subprocess.STDOUT)\n",
    "        process.wait()\n",
    "        logfile.flush()\n",
    "        print(f\"Locust testing completed\")\n",
    "except Exception as e:\n",
    "    print(f\"exception occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Locust Results\n",
    "\n",
    "Below we first visualize the results of the Locust testing in a tabel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "\n",
    "results_path = os.environ[\"RESULTS_PREFIX\"] + \"_stats.csv\"\n",
    "df = pd.read_csv(results_path)\n",
    "df = df.replace(np.nan, '')\n",
    "\n",
    "top_n = 1\n",
    "caption=f\"Locust results\"\n",
    "df = df.truncate(after=top_n - 1, axis=0)\n",
    "df = df.style \\\n",
    "      .format(precision=6) \\\n",
    "        .set_properties(**{'text-align': 'left'}) \\\n",
    "        .set_caption(caption)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shutdown inference server\n",
    "\n",
    "Next, we shutdown inference server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! {script_path} down"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "aws_neuron_venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
